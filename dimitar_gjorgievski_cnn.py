# -*- coding: utf-8 -*-
"""Dimitar_Gjorgievski_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rAATB8xbKXf01ajlWzJNmw2gKEvfgQyS

Name: Dimitar Gjorgievski
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')
from IPython import display
import pandas as pd
import tensorflow as tf
import os.path
#if os.path.isfile('/content/drive/MyDrive/cs412/CNN/saved_mode.h5') is False

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/cs412/CNN

## helper funciton for loading the horses data
#make sure the horses data is in the same directory as the notebook
#do not change this function
def load_horses_orig(path, image_size):
    mask_path = path + 'masks/'
    image_path = path + 'images/'
    images = []
    masks = []
    test_images= []
    test_masks =[]
    for i in range(328):

        orig_im = cv2.imread(image_path + 'image-{}.png'.format(i))
        orig_im= cv2.cvtColor(orig_im, cv2.COLOR_RGB2BGR)

        low_im = cv2.resize(orig_im, dsize=(image_size, image_size))

        orig_mask = cv2.imread(mask_path + 'mask-{}.png'.format(i))
        low_mask = cv2.resize(orig_mask, dsize=(image_size, image_size))
        low_mask = cv2.cvtColor(low_mask, cv2.COLOR_RGB2GRAY)
        bin_mask = (low_mask > 0) + 0


        images.append(low_im)
        masks.append(bin_mask)


    xtest = np.reshape(np.array(images[250:]), (-1,image_size*image_size*3))
    ytest = np.reshape(np.array(masks[250:]), (-1, image_size * image_size))
    xdata = np.reshape(np.array(images[:200]), (-1,image_size*image_size*3))
    ydata = np.reshape(np.array(masks[:200]), (-1, image_size * image_size))
    yval =  np.reshape(np.array(masks[200:250]), (-1, image_size * image_size))
    xval = np.reshape(np.array(images[200:250]), (-1,image_size*image_size*3))


    return xdata, xval, xtest, ydata, yval, ytest

#change the path address if put the data somewhere else
path = '/content/drive/MyDrive/cs412/CNN/horses/'
image_size = 32;
xdata, xval, xtest, ydata, yval, ytest = load_horses_orig(path, image_size)

#helper function to drawing horse and its mask
def draw(image, mask):
    fig, (ax1,ax2) = plt.subplots(1,2)
    ax1.axis('off')
    ax2.axis('off')
    ax1.imshow(np.reshape(image, (image_size,image_size,3)))
    ax2.imshow(np.reshape(mask, (image_size,image_size)), cmap=plt.cm.gray)
    plt.show()

"""The task is to predict the mask for the horse given the image
Mask is binary image shows the presence of the horse.
"""

draw(xdata[0], ydata[0])

#Helper function for data augmentation
def add_salt_and_pepper_noise(image, salt_prob, pepper_prob):
    salt_mask = np.random.rand(*image.shape) < salt_prob
    pepper_mask = np.random.rand(*image.shape) < pepper_prob

    salt_value = 1.0
    pepper_value = 0.0

    noisy_image = np.where(salt_mask, salt_value, image)
    noisy_image = np.where(pepper_mask, pepper_value, noisy_image)

    return noisy_image

for i in range(xdata.shape[0]):
    xdata[i] =  add_salt_and_pepper_noise(xdata[i], salt_prob=0.01, pepper_prob=0.01)


draw(xdata[0], ydata[0])

train_size = xdata.shape[0]
batch_size = 10
train_dataset = (tf.data.Dataset.from_tensor_slices(np.hstack((xdata, ydata)))
                 .shuffle(train_size).batch(batch_size))

"""The intersection over union (IOU) is a metric for measuring the performance of image segmentation. The perfect segmentation receives IOU of one."""

# Do not change this cell
def iou(ytrue, yprediction):
    yp = yprediction
    yt = ytrue
    yp = yp > 0.5 + 0
    intersect = np.sum(np.minimum(yp, yt),1)
    union = np.sum(np.maximum(yp, yt),1)
    return np.average(intersect / (union+0.0))

assert iou(ydata, ydata) == 1.0

"""We can use feedforwad MLP or CNN  model for image segmentation. Here the input is the image and the output is the segmentation mask."""

class NeuralNet(tf.keras.Model):

    def construct_CNN(self):
        # COMPLETE ME
        self.model = tf.keras.Sequential(
          [
            # Input Layer
            tf.keras.layers.InputLayer(input_shape=(image_size, image_size, 3)),

            # Convolution layers
            tf.keras.layers.Conv2D(5, (3,3), activation='relu', padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2D(5, (3,3), activation='relu', padding='same'),
            tf.keras.layers.BatchNormalization(),


            # Transposed convolution layers for upsampling
            tf.keras.layers.Conv2DTranspose(3, (3,3), activation='relu', padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2DTranspose(1, (3,3), activation='sigmoid', padding='same'),


            # Output Layer
            tf.keras.layers.Flatten()

          ]
        )


    def construct_MLP(self):
         self.model = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(input_shape=(image_size, image_size, 3), dtype='float32'),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(500, activation='relu'),
                tf.keras.layers.Dropout(.2, input_shape=(500,)),
                tf.keras.layers.Dense(20, activation='relu'),
                tf.keras.layers.Dense(image_size*image_size)
            ]
        )


    def __init__(self, network_type = 'MLP'):
        super(NeuralNet, self).__init__()
        #DOCUMENT ME
        #This code initializes the optimizer to be used in this model.
        #The initial optimizer was stochastic gradient descent,
        #but there are also other alternitives that tend to work better on this model.
        #Some other to consider are Adam, Nadam & Adagrad.
        #They each have a specific algorithm with how they apply the updates of the parameters, and will produce different results.

        #self.optimizer = tf.keras.optimizers.SGD(0.1)
        #What are the other available optimizers
        #Test your code with different optimizers

        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        #self.optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)
        #self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)

        if network_type == 'MLP':
            self.construct_MLP();
        else:
            self.construct_CNN();


    #DOCUMENT ME
    #The function does the forward pass of the model.
    #When predict is true we are apllying the sigmoid activation function to the output of the neural network.
    #Thereby, making a prediction.
    #When predict is false this function returns the raw output of the neural network, without making a prediction.
    #This output is used in the loss function
    def forward(self, x, predict=True):
        if predict:
            return tf.nn.sigmoid(self.model(x))
        return self.model(x)

    def get_loss(self, x, yt):
        x = tf.reshape(x, (-1, image_size, image_size, 3))
        yt = tf.reshape(yt, (-1, image_size * image_size))
        ylogits = self.forward(x, predict=False);

        #DOCUMENT ME
        #With the raw output retreived from the neural network, we calculate the loss function.
        #In this case it is the binary cross entropy loss.
        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=ylogits, labels=yt);


        return tf.reduce_sum(cross_ent)


    def train_step(self, xbatch,ybatch):

        #DOCUMENT ME
        #This operation is executed during the training process.
        #This code makes a forward pass of the model and gets the loss.
        #This code is placed inside a with statement so it can record the trainable parameters used during the forward pass
        with tf.GradientTape() as tape:
            loss = self.get_loss(xbatch, ybatch)

        #DOCUMENT ME
        #Since the trainable parameters were recorded on the tape,
        #it can calculate the gradient of the loss with respect to those trainable parameters.
        #Using the tape.gradient() function, the variable gradient holds a tensor representing the gradients of the loss
        #with respect to each trainable parameter in model.trainable_variables.
        #This code calculates the backpropagation.
        gradients = tape.gradient(loss, model.trainable_variables)

        #DOCUMENT ME
        #This code updates the parameters with the calculated gradients.
        #The instance of the optimizer gets the computed gradients and their corresponding parameters.
        #Using the zip function it creates tuples by pairing each gradient and parameter.
        #Finally, the apply_gradients() function of the optimizer applies the gradients to the parameters,
        #with respect to the algorithm of the optimizer.
        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        return tf.reduce_sum(loss)

    def predict(self, x):
        y = self.forward(x, predict=True)
        return y.numpy() #tf.reshape(y, (-1, image_size, image_size)).numpy()

model = NeuralNet('CNN')
model.model.summary()
#model = NeuralNet('MLP')
train_iou = []
val_iou = []
test_iou = []
epoch = 1;
best_val_iou = -1;
best_test_iou = -1

#Adopt similar training loop for other problems

max_epoch = 5000

while epoch < max_epoch:
    loss = 0.0
    for batch in train_dataset:
        xbatch = batch[:,:image_size*image_size*3]
        ybatch = batch[:,image_size*image_size*3:]
        xbatch = tf.cast(xbatch, tf.float32);
        ybatch = tf.cast(ybatch, tf.float32);
        loss += model.train_step(xbatch, ybatch);

    ydata_pred = model.predict(tf.reshape(xdata, (-1, image_size, image_size, 3 )));
    yval_pred = model.predict(tf.reshape(xval, (-1, image_size, image_size, 3 )));
    ytest_pred = model.predict(tf.reshape(xtest, (-1, image_size, image_size, 3 )));

    train_iou.append(iou(ydata,ydata_pred))
    val_iou.append(iou(yval,yval_pred))
    test_iou.append(iou(ytest,ytest_pred))

    if val_iou[-1] > best_val_iou:
        best_test_iou = test_iou[-1]
        best_val_iou = val_iou[-1]
        model.save_weights("best_model");
        #model.save('/content/drive/MyDrive/cs412/CNN/saved_mode.h5')

    print("Epoch {}, Loss: {:0.3}, IOU - Train: {:0.3} Valid: {:0.3} Test: {:0.3}".format( epoch, loss.numpy(), train_iou[-1], val_iou[-1], best_test_iou))



    epoch += 1

fig, ax = plt.subplots(1,1)
pd.Series(train_iou).plot(ax=ax)
pd.Series(val_iou).plot(ax=ax)
pd.Series(test_iou).plot(ax=ax)
fig.canvas.draw()

draw(xdata[0], ydata_pred[0])

model.load_weights("best_model")
ypred = model.predict(tf.reshape(xtest, (-1, image_size, image_size, 3 )));

draw(xtest[10], ypred[10])
draw(xtest[25], ypred[25])
draw(xtest[20], ypred[20])

iou(ytest, ypred)

